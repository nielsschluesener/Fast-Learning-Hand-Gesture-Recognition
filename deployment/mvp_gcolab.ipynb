{"cells":[{"cell_type":"markdown","source":["# New to this MVP? "],"metadata":{"id":"sfabcGbMyM-X"}},{"cell_type":"markdown","source":["### Watch my [Tutorial](https://youtu.be/WzGw5L0Y_N8) on YouTube before you start!"],"metadata":{"id":"svzwqKRFyuzD"}},{"cell_type":"markdown","metadata":{"id":"yYbILxN9xsjU"},"source":["# Step No 1 - Make sure you have GPU enabled."]},{"cell_type":"markdown","metadata":{"id":"pjeWLIZ82q14"},"source":["**Before you can with the application start:** Run the following code and make sure it lists GPU, not CPU or TPU. If it doesn't, go to \"Runtime\" -> \"Change runetime type\", select \"GPU\" and restart the notebook (should be done automatically)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_S-eVm1jyRsc"},"outputs":[],"source":["!nvidia-smi"]},{"cell_type":"markdown","metadata":{"id":"LwNpi9Bcf_20"},"source":["# Step No 2 - Initialize the application."]},{"cell_type":"markdown","metadata":{"id":"xn6cdiR28HF8"},"source":["Next, you have to initiate the application. Therefore, hit the play-button below the following section-header. If you would like to debug the application or take a look into the magic, expand the section-header."]},{"cell_type":"markdown","metadata":{"id":"HnUoZav9Lzf-"},"source":["### Hit the play-button below this line."]},{"cell_type":"markdown","metadata":{"id":"Lln_jZx3gpaO"},"source":["#### Import Libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mUK33RKxIA-P"},"outputs":[],"source":["import pandas as pd\n","import os\n","import cv2 as cv\n","import numpy as np\n","import PIL\n","import io\n","import html\n","import time\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import pickle\n","from torch.autograd import Variable\n","from google.colab import drive\n","from google.colab.output import eval_js\n","from google.colab import output\n","from base64 import b64decode\n","from tabulate import tabulate\n","from IPython.display import display, Javascript\n","\n","os.system(\"\"\"pip install mediapipe --quiet\"\"\")\n","import mediapipe as mp\n","\n","os.system(\"\"\"pip install alive-progress -q\"\"\")\n","from alive_progress import alive_bar"]},{"cell_type":"markdown","metadata":{"id":"Y-6tV3692ExT"},"source":["#### Assign device"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hMI-DxutyXSU"},"outputs":[],"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"]},{"cell_type":"markdown","metadata":{"id":"rM2qW0ecgwu3"},"source":["#### Initialize functions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YvIlWzychHmQ"},"outputs":[],"source":["def initialize_model(n_way, k_shot, classes):\n","  \"\"\"\n","  main function to initialize both model components based on a given n_way, k_shot scenario\n","  Args:\n","    n_way (int): Number of different classes to be classfied\n","    k_shot (int): Number of support samples per class\n","    classes (list): Names of classes to predict\n","  Returns:\n","    feature_encoder (LSTMEncoder-object): initialized LSTMEncoder-object\n","    relation_network (RelationNetwork-object): initialized RelationNetwork-object\n","  \"\"\"\n","\n","  # Check if classes-length match n_way\n","  if n_way != len(classes):\n","    print('Error: The size of your list of gestures does not match N_WAY. Please correct this and restart the process.')\n","    return None, None\n","\n","  # Clone github repo where the model is stored\n","  succes = os.system(\"\"\"git clone https://github.com/nielsschluesener/S-STRHanGe.git\"\"\")\n","\n","  # define model_name and its path\n","  model_name = f'{n_way}way-{k_shot}shot'\n","  model_path = 'S-STRHanGe/deployment/models/'\n","\n","  # get deployment parameters\n","  if os.path.isfile(os.path.join(model_path, f'{model_name}_deployment_param.pkl')):\n","    with open(os.path.join(model_path, f'{model_name}_deployment_param.pkl'), 'rb') as f:\n","      deployment_param = pickle.load(f)\n","\n","  # create feature_encoder and relation_network objects with params given by deployment_params\n","  feature_encoder = LSTMEncoder(deployment_param['feature_length'], deployment_param['num_units_lstm_encoder'], deployment_param['num_lstm_layer_encoder']).to(device)\n","  relation_network = RelationNetwork(deployment_param['num_units_lstm_encoder']*2, deployment_param['num_units_lstm_relationnet'], deployment_param['num_units_fc_relu'], deployment_param['num_lstm_layer_relationnet']).to(device) \n","\n","  # load the trained weights\n","  feature_encoder.load_state_dict(torch.load(os.path.join(str(os.path.join(model_path, f'{model_name}_feature_encoder.pkl')))))\n","  relation_network.load_state_dict(torch.load(os.path.join(str(os.path.join(model_path, f'{model_name}_relation_network.pkl')))))\n","\n","  return feature_encoder, relation_network\n","\n","\n","class LSTMEncoder(nn.Module):\n","  \"\"\"First part of the model, which encodes the sequence of hand gesture keypoints\"\"\"\n","  def __init__(self, input_size, hidden_lstm_size, num_lstm_layer):\n","    \"\"\"\n","    Object initialization.\n","    Args:\n","      input_size (int): size of the input data\n","      hidden_lstm_size (int): number of units in lstm-cell\n","      num_lstm_layer (int): number of stacked lstm-layers\n","    \"\"\"\n","    super(LSTMEncoder, self).__init__()\n","    self.input_size = input_size \n","    self.hidden_lstm_size = hidden_lstm_size \n","    self.num_lstm_layer = num_lstm_layer\n","\n","    #create lstm layer(s)\n","    self.lstm = nn.LSTM(input_size = input_size, hidden_size = hidden_lstm_size, num_layers = num_lstm_layer, batch_first=True)\n","\n","  def forward(self,x):\n","    \"\"\"\n","    Forward pass through the LSTM Encoder\n","    Args:\n","      x (torch tensor): data input (sequence of hand keypoints as batch)\n","    Return:\n","      out (torch tensor): encoded data (encoded sequence of hand keypoints as batch) with size batch, sequence, hidden_size\n","    \"\"\"\n","\n","    #create zeros with size num_layer, sequences, hidden_size \n","    #h0 containing the final hidden state for each element in the sequence\n","    h0 = torch.zeros(self.num_lstm_layer, x.size(0), self.hidden_lstm_size).to(device)  \n","    #c0 containing the final cell state for each element in the sequence\n","    c0 = torch.zeros(self.num_lstm_layer, x.size(0), self.hidden_lstm_size).to(device) \n","\n","    #pass data through the lstm layer(s)\n","    out, _ = self.lstm(x, (h0,c0))\n","    return out \n","\n","\n","class RelationNetwork(nn.Module):\n","  \"\"\"Second part of the model, which calculates the relation-scores, given the concatenated query and support samples\"\"\"\n","  def __init__(self,input_size, hidden_lstm_size, fc_sizes: list, num_lstm_layer):\n","    \"\"\"\n","    Object initialization.\n","    Args:\n","      input_size (int): size of the input data\n","      hidden_lstm_size (int): number of units in lstm-cell\n","      fc_sizes (list): number of units per fully connected / linear layer\n","      num_lstm_layer (int): number of stacked lstm-layers\n","    \"\"\"\n","    super(RelationNetwork, self).__init__()\n","    self.input_size = input_size \n","    self.hidden_lstm_size = hidden_lstm_size\n","    self.layers = nn.ModuleList()\n","    self.num_lstm_layer = num_lstm_layer\n","\n","    #create lstm layer(s)\n","    self.lstm = nn.LSTM(input_size = input_size, hidden_size = hidden_lstm_size, num_layers = num_lstm_layer, batch_first=True)\n","    #next input_size is outputsize of the lstm layer (hidden_lstm_size)\n","    input_size = hidden_lstm_size\n","\n","    #loop over fc_sizes, create linear layer + ReLU Activation and adjust the input_size\n","    for size in fc_sizes:\n","      self.layers.append(nn.Linear(input_size, size))\n","      self.layers.append(nn.ReLU())\n","      input_size = size  \n"," \n","    #add a final Linear Layer which reduces the input down to a single number, aka. the relation score, by applieing a sigmoid function\n","    self.end_layer = nn.Sequential(nn.Linear(input_size,1), nn.Sigmoid())\n","    \n","  def forward(self,x):\n","    \"\"\"\n","    Forward pass through the Relation Network\n","    Args:\n","      x (torch tensor): concatenated data input of query- and support-samples \n","    Return:\n","      out (torch tensor): relation scores\n","    \"\"\"\n","    \n","    #create zeros with size num_layer, sequences, hidden_size \n","    #h0 containing the final hidden state for each element in the sequence \n","    h0 = torch.zeros(self.num_lstm_layer, x.size(0), self.hidden_lstm_size).to(device) \n","    #c0 containing the final cell state for each element in the sequence\n","    c0 = torch.zeros(self.num_lstm_layer, x.size(0), self.hidden_lstm_size).to(device) \n","\n","    #pass data through the lstm layer(s)\n","    out, _ = self.lstm(x, (h0,c0))\n","    #reduce output to the last sequence-state \n","    out = out[:, -1, :]\n","\n","    #pass data through the fc layer(s)\n","    for layer in self.layers:\n","      out = layer(out)\n","    \n","    #pass data through the final layer to calculate the relation score\n","    out = self.end_layer(out) \n","    return out\n","\n","def create_support_set(n_way, k_shot, classes):\n","  \"\"\"\n","  main function to create a support set for a given n_way, k_shot scenario \n","  Args:\n","    n_way (int): Number of different classes to be classfied\n","    k_shot (int): Number of support samples per class\n","  Returns:\n","    support_x (torch.tensor): keypoint data of the support sample\n","  \"\"\"\n","\n","  # Create supportset-process-logging\n","  indexes = []\n","  for i in range(1, K_SHOT+1):\n","    indexes.append(f'#{i}')\n","  supset_state = pd.DataFrame(np.empty([K_SHOT, N_WAY], dtype = str), index = indexes, columns = GESTURES)\n","\n","  # Initialize supset dir\n","  supset_dir = 'support_set'\n","  support_x, support_y = initialize_supset_dir(n_way, k_shot, supset_dir)\n","  if support_x != None:\n","    return support_x, support_y\n","\n","  # Loop over the classes and create k_shot-videos\n","  for n in range(1, n_way+1):\n","    for s in range(1, k_shot + 1):\n","      # print state and instructions\n","      supset_state.iloc[s-1,n-1] = '''This turn's gesture!'''\n","      output.clear()\n","      print('Current state your your support-set: \\n')\n","      print(tabulate(supset_state, headers = supset_state.columns, tablefmt = 'simple', stralign=\"center\"))\n","      print(f'''\\nWait until the green countdown hits 0 and perform your gesture \"{str(classes[n-1])}\" into your webcam!\\n''')\n","      time.sleep(5) if n == 1 and s == 1 else time.sleep(2)\n","      \n","      log = record_video(supset_dir + f'/class{n}_sample{s}/video/gesture.mp4') \n","      if log == False:\n","        print('Video recording failed.')\n","        return None, None\n","\n","      #Update supset_state\n","      supset_state.iloc[s-1,n-1] = '✓'\n","\n","  output.clear()\n","  print('All videos have been captured. \\n')\n","  print(tabulate(supset_state, headers = supset_state.columns, tablefmt = 'simple', stralign=\"center\"))\n","  print('\\nThe keypoints are now being extracted. This may take a few minutes dependent on the size of the support set. \\n')\n","\n","  # Loop over the videos and extract keypoints\n","  with alive_bar(n_way*k_shot, title=f'Extracting keypoints...', force_tty = True) as bar:\n","    for n in range(1, n_way+1):\n","      for s in range(1, k_shot + 1):\n","        extract_supset_frames_from_video(n, s, supset_dir)\n","        generate_supset_keypoints_from_frame(n, s, supset_dir)\n","        bar()\n","\n","  # generate support_x and support_y from keypoints selected\n","  support_x, support_y = create_supset_from_keypoints(n_way, k_shot, supset_dir)\n","\n","  print('\\nAll keypoints have been extracted. Your support set is ready to go!')\n","\n","  return support_x\n","\n","def record_video(filename='video.mp4'):\n","  js = Javascript(\"\"\"\n","    async function recordVideo() {\n","\n","      const div = document.createElement('div');\n","      const capture = document.createElement('button');\n","      const stopCapture = document.createElement(\"button\");\n","\n","      // Define a output element for debugging purposes\n","      const errorLog = document.createElement(\"div\");\n","\n","      capture.textContent = \"Recording starts in 3\";\n","      capture.style.background = \"green\";\n","      capture.style.color = \"white\";\n","\n","      stopCapture.textContent = \"Recording ends in 3\";\n","      stopCapture.style.background = \"red\";\n","      stopCapture.style.color = \"white\";\n","      div.appendChild(capture);\n","\n","      const video = document.createElement('video');\n","      const recordingVid = document.createElement(\"video\");\n","      video.style.display = 'block';\n","\n","      // Disable audio capture and limit to video\n","      const stream = await navigator.mediaDevices.getUserMedia({audio: false, video: true });\n","      // create a media recorder instance, which is an object\n","      // that will let you record what you stream.\n","\n","      const options = {\n","        videoBitsPerSecond : 2500000,\n","        mimeType: \"video/webm;codecs=vp8\" //9 \n","      }\n","\n","      let recorder = new MediaRecorder(stream, options);\n","      document.body.appendChild(div);\n","      div.appendChild(video);\n","\n","      // Append output element for debugging\n","      div.appendChild(errorLog);\n","\n","      // Debugging only: Give back the videoBitsPerSecond of the option defined in the MediaRecorder constructor\n","      // errorLog.textContent = recorder.videoBitsPerSecond;  \n","\n","      // Create a media element.  \n","      video.srcObject = stream;\n","      await video.play();\n","\n","      // Resize the output to fit the video element.\n","      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n","      \n","      // start recording\n","      // Defines the time left for the countdown; just for fun not necessary\n","      var recordingCountdown = 3; \n","      var preCountdown = 3;\n","\n","    // Pre Countdown Timer start ; just for fun not necessary\n","        var preCountdownTimer = setInterval(function(){ \n","        preCountdown--; \n","        capture.textContent = \"Recording starts in \"+ preCountdown; \n","        // if preCountdown is <= 0 run recording function\n","        if(preCountdown <= 0){\n","            clearInterval(preCountdownTimer);\n","            startRecording();}\n","        },1000);\n","      // Pre Countdown Timer end ; just for fun not necessary\n","\n","      // function which runs the recording\n","      function startRecording() {\n","\n","        // switches the buttons to stopCapture one, just visually, \n","        // stopCapture as no function anymore\n","        capture.replaceWith(stopCapture);\n","\n","        // starts recording\n","        recorder.start();\n","\n","        // Recording Countdown Timer start ; just for fun not necessary\n","        var recordingCountdownTimer = setInterval(function(){ \n","        recordingCountdown--; \n","        stopCapture.textContent = \"Recording ends in \"+recordingCountdown; \n","        if(recordingCountdown <= 0){ \n","            clearInterval(recordingCountdownTimer);}\n","        },1000);\n","      // Recording Countdown Timer end ; just for fun not necessary\n","\n","      // sets delay/timeout of 3000ms to automatically stop recording\n","      setTimeout(event => {\n","          recorder.stop();\n","        }, 3000);\n","      };\n","      \n","      let recData = await new Promise((resolve) => recorder.ondataavailable = resolve);\n","      let arrBuff = await recData.data.arrayBuffer();\n","      \n","      // stop the stream and remove the video element\n","      stream.getVideoTracks()[0].stop();\n","      div.remove();\n","\n","      let binaryString = \"\";\n","      let bytes = new Uint8Array(arrBuff);\n","      bytes.forEach((byte) => {\n","        binaryString += String.fromCharCode(byte);\n","      })\n","      return btoa(binaryString);\n","    }\n","    \"\"\")\n","  try:\n","    display(js)\n","    data = eval_js('recordVideo({})')\n","    binary = b64decode(data)\n","    with open(filename, \"wb\") as video_file:\n","      video_file.write(binary)\n","  except Exception as err:\n","      # In case any exceptions arise\n","      print(str(err))\n","      return False\n","  return filename\n","\n","def extract_keypoints(results):\n","  \"\"\"\n","  Extracts keypoints from a given mediapipe-Hands-result\n","  Args:\n","    results (mp-hands-object): result of a mediapipe-Hands hand-detection\n","  Returns:\n","    keypoints (np array): 63 keypoints + hand-indicator + classification-score or 65 zeros\n","  \"\"\"\n","  # If the detection was succesfull, return 63 keypoints + hand-indicator and classification-score, else return 65 zeros\n","  if results.multi_hand_landmarks:\n","    landmarks = np.array([[res.x, res.y, res.z] for res in results.multi_hand_landmarks[0].landmark ]).flatten() if results.multi_hand_landmarks else np.zeros(63)\n","    return landmarks\n","  else:\n","    return np.zeros(63)\n","\n","def detect_hand(image_path):\n","  \"\"\"\n","  Runs the standard mediapipe-hand-detection model on a single image\n","  Args:\n","    image_path (str): path to an image\n","  Returns:\n","    result (mp-hands-object): result of a mediapipe-Hands hand-detection\n","  \"\"\"\n","  #Run the mp_hands.Hands-Model with standard-Params (one Hand max, min_confidence of 0.5) on a flipped image (because webcam images are flipped aswell)\n","  with mp.solutions.hands.Hands(\n","      static_image_mode = True,\n","      max_num_hands = 1,\n","      min_detection_confidence = 0.5) as hands:\n","      image = cv.flip(cv.imread(image_path), 1)\n","      result = hands.process(cv.cvtColor(image, cv.COLOR_BGR2RGB))\n","  return result\n","\n","def move_values(data, seq_length, direction = 'back'):\n","  \"\"\"\n","  Move values in each sequence either towards the back of the sequence or towards the front.\n","  Args:\n","    data (np.array): Data instance, usually x\n","    seq_length (int): Length of sequence\n","    direction (str): target direction, either 'back' (values move towards the back) or 'front' (values move towarrds the front)\n","  Returns:\n","    x (np.array): Instance (instance of data) with values moved towards the intended direction\n","  \"\"\"\n","\n","  #Make sure the instance is not empty\n","  max_values = data.max(axis = 1)\n","  if np.array_equal(max_values,np.zeros(seq_length)) == True:\n","    return np.zeros((seq_length, 63))\n","  #Else, compute the index of the first keypoints and the last keypoints\n","  else:\n","    i_first_value = next(i for i,v in enumerate(max_values) if v != 0)\n","    i_last_value = next(i for i,v in reversed(list(enumerate(max_values))) if v != 0)\n","\n","    #create temp datasets which holds the values to be moved and the array of zeros, with which the sequence is padded\n","    non_zeros = data[i_first_value:i_last_value+1]\n","    zeros = np.zeros((seq_length - non_zeros.shape[0], 63))\n","\n","    #Move the values\n","    if direction == 'back':\n","      x = np.concatenate((zeros, non_zeros), axis=0)\n","    elif direction == 'front':\n","      x = np.concatenate((non_zeros, zeros), axis=0)\n","    \n","    return x\n","\n","def initialize_supset_dir(n_way, k_shot, supset_dir):\n","  \"\"\"\n","  Initialiazes the support_set directory\n","  Args:\n","    n_way (int): Number of different classes to be classfied\n","    k_shot (int): Number of support samples per class\n","    supset_dir (str): Target name of the support set directory\n","  \"\"\"\n","\n","  # Make sure the user does not delete the current supset_dir with its values unintentionally\n","  if os.path.exists(supset_dir):\n","    safety_check = input (\"Caution: This step will delete the old support set. Do you want to continue? Type Yes or No and hit Enter.\") \n","    if safety_check == 'Yes':\n","      log = os.system(\"\"\"rm -rf support_set\"\"\")\n","      output.clear()\n","    elif safety_check == 'No':\n","      support_x, support_y = create_supset_from_keypoints(n_way, k_shot, supset_dir)\n","      print('Procedure canceled. Old support set was preserved.')\n","      return support_x, support_y\n","\n","  # Else, create support_set directory with sub_directories\n","  if not os.path.exists(supset_dir):\n","    os.makedirs(supset_dir)\n","    for n in range(1, n_way+1):\n","      for s in range(1, k_shot + 1):\n","        os.makedirs(supset_dir + f'/class{n}_sample{s}')\n","        os.makedirs(supset_dir + f'/class{n}_sample{s}/video')\n","        os.makedirs(supset_dir + f'/class{n}_sample{s}/frames')\n","        os.makedirs(supset_dir + f'/class{n}_sample{s}/keypoints')\n","\n","  return None, None\n","\n","def extract_supset_frames_from_video(n_way, k_shot, supset_dir):\n","  \"\"\"\n","  Extracts frames from given support-set video, defined by its class and k'th-shot\n","  Args:\n","    n_way (int): class of the video\n","    k_shot (int): k'th-shot of the video\n","    supset_dir (str): Support set directory\n","  \"\"\"\n","\n","  # If video exists, turn video into jpgs\n","  if os.path.isfile(supset_dir + f'/class{n_way}_sample{k_shot}/video/gesture.mp4'):\n","    os.system(f''' ffmpeg -i support_set/class{n_way}_sample{k_shot}/video/gesture.mp4 -qscale:v 2 -vf \"scale=360:240,fps=24\" support_set/class{n_way}_sample{k_shot}/frames/frame_%02d.jpg ''')\n","\n","def generate_supset_keypoints_from_frame(n_way, k_shot, supset_dir):\n","  \"\"\"\n","  Generated and saves keypoints from given supset-frames, defined by its class and k'th-shot\n","  Args:\n","    n_way (int): class of the frames\n","    k_shot (int): k'th-shot of the frames\n","    supset_dir (str): Support set directory\n","  \"\"\"\n","  num_frames = 72\n","  mp_hands = mp.solutions.hands\n","\n","  # Loop over frames\n","  for frame_num in range(1, num_frames + 1):\n","  # Generate keypoints and save as npy\n","    if os.path.isfile(supset_dir + f'/class{n_way}_sample{k_shot}/frames/frame_{str(frame_num).zfill(2)}.jpg'):\n","      hand = detect_hand(supset_dir + f'/class{n_way}_sample{k_shot}/frames/frame_{str(frame_num).zfill(2)}.jpg')\n","      keypoints = extract_keypoints(hand)\n","      np.save(supset_dir + f'/class{n_way}_sample{k_shot}/keypoints/frame_{str(frame_num).zfill(2)}.npy', keypoints)\n","\n","def create_supset_from_keypoints(n_ways, k_shots, supset_dir):\n","  \"\"\"\n","  Collects and concats all keypoints from a given subset_dir\n","  Args:\n","    n_way (int): number of classes\n","    k_shot (int): number of \n","    supset_dir (str): Support set directory\n","  Returns:\n","    support_x (torch.tensor): keypoint data of the support sample\n","    support_y (torch.tensor): labels of the support sample\n","  \"\"\"\n","\n","  num_frames = 72\n","  X, y = [], []\n","\n","  # Loop over every supset-sample, and collect X and y data\n","  for n in range(1, n_ways+1):\n","    for s in range(1, k_shots + 1):\n","      sequence = []\n","      for frame in range (1, num_frames + 1):\n","        if os.path.isfile(supset_dir + f'/class{n}_sample{s}/keypoints/frame_{str(frame).zfill(2)}.npy'):\n","          keypoints = np.load(supset_dir + f'/class{n}_sample{s}/keypoints/frame_{str(frame).zfill(2)}.npy')\n","          sequence.append(keypoints)\n","        else:\n","          sequence.append(np.zeros(63))\n","\n","      X.append(sequence)\n","      y.append(n)\n","\n","  # create np.arrays for X and y and move values to the back \n","  X = np.array(X)\n","  y = np.array(y)\n","\n","  for i in range(len(X)):\n","    X[i] = move_values(X[i], num_frames)\n","\n","  # turn X and y to torch tensors\n","  support_x, support_y  = torch.from_numpy(X).float(), torch.from_numpy(y).float().type(torch.LongTensor)\n","\n","  return support_x, support_y\n","\n","def perform_prediction(support_x, classes, feature_encoder, relation_network):\n","  \"\"\"\n","  Main function for performing a prediction \n","  Args:\n","    support_x (torch.tensor): keypoint data of the support sample\n","    classes (list): names of the classes, defined by the user\n","    feature_encoder (LSTMEncoder-object): initialized LSTMEncoder-object\n","    relation_network (RelationNetwork-object): initialized RelationNetwork-object\n","  \"\"\"\n","  # load deployment_params\n","  model_name = f'{len(classes)}way-{int(len(support_x) / len(classes))}shot'\n","  model_path = 'S-STRHanGe/deployment/models/'\n","  if os.path.isfile(os.path.join(model_path, f'{model_name}_deployment_param.pkl')):\n","    with open(os.path.join(model_path, f'{model_name}_deployment_param.pkl'), 'rb') as f:\n","      deployment_param = pickle.load(f)\n","\n","  # generate x-data\n","  x = create_prediction_data()\n","\n","  print('Predicting... \\n')\n","  time.sleep(1)\n","  # Calculate relations\n","  relations = calc_prediction(feature_encoder = feature_encoder, relation_network = relation_network, \n","                              support_x = support_x, query_x = x, \n","                              num_classes = deployment_param['num_classes'], support_num_per_class = deployment_param['support_num_per_class'], \n","                              seq_length = deployment_param['sequence_length'], num_units_lstm_encoder = deployment_param['num_units_lstm_encoder'])\n","\n","  print('Prediction finished! \\n')\n","\n","  # Output results\n","  print(f'The model predicts: {classes[torch.argmax(relations).item()]}\\n')\n","\n","  print('The calculated relation scores to all classes are:\\n')\n","  print(tabulate(relations.tolist(), tablefmt = 'simple', headers=classes, numalign=\"right\", floatfmt=\".2f\"))\n","  print('\\nThe higher the score, the more similarity the model sees between your gesture and the gestures of that class in the support-set.')\n","\n","  print('\\nRerun the cell to perform another prediction!')\n","\n","def create_prediction_data():\n","  \"\"\"\n","  Creates the data neccesary for a prediction\n","  Returns:\n","    x (torch.tensor): keypoint data to predict\n","  \"\"\"\n","  \n","  predict_dir = 'prediction'\n","  num_frames = 72\n","\n","  # Create or empty prediction-directory \n","  if os.path.exists(predict_dir):\n","    log = os.system(\"\"\"rm -rf prediction\"\"\")\n","  \n","  os.makedirs(predict_dir)\n","  os.makedirs(predict_dir + f'/video')\n","  os.makedirs(predict_dir + f'/frames')\n","  os.makedirs(predict_dir + f'/keypoints')\n","\n","\n","  # Record the video\n","  print(f'''Wait until the green countdown hits 0 and perform the gesture you want the model to detect into your webcam!''')\n","  time.sleep(5) \n","  log = record_video(predict_dir + '/video/gesture.mp4')\n","  if log == False:\n","    print('Video recording failed.')\n","    return None\n","\n","  output.clear()\n","  print('Video has been captured. \\n')\n","\n","  # Turn video into frames\n","  if os.path.isfile(predict_dir + f'/video/gesture.mp4'):\n","    os.system(f''' ffmpeg -i {predict_dir}/video/gesture.mp4 -qscale:v 2 -vf \"scale=360:240,fps=24\" {predict_dir}/frames/frame_%02d.jpg ''')\n","\n","  sequence = []\n","  #Generate keypoints from frames and save as npy\n","  with alive_bar(num_frames, title=f'Extracting keypoints...', force_tty = True) as bar:\n","    for frame_num in range(1, num_frames + 1):\n","      if os.path.isfile(predict_dir + f'/frames/frame_{str(frame_num).zfill(2)}.jpg'):\n","        hand = detect_hand(predict_dir + f'/frames/frame_{str(frame_num).zfill(2)}.jpg')\n","        keypoints = extract_keypoints(hand)\n","        np.save(predict_dir + f'/keypoints/frame_{str(frame_num).zfill(2)}.npy', keypoints)\n","        sequence.append(keypoints)      \n","      else:\n","        sequence.append(np.zeros(63))\n","      bar()\n","  print('\\nKeypoints have been extracted. \\n')\n","\n","  # Generate x\n","  x = np.array(sequence)\n","  x = move_values(x, x.shape[0])\n","  x = np.expand_dims(x, axis = 0)\n","  x = torch.from_numpy(x).float()\n","\n","  return x\n","\n","def calc_prediction(feature_encoder, relation_network, support_x, query_x, num_classes, support_num_per_class, seq_length, num_units_lstm_encoder):\n","  \"\"\"\n","  Prediction-Function \n","  Args:\n","    feature_encoder (LSTMEncoder-object): initialized LSTMEncoder-object\n","    relation_network (RelationNetwork-object): initialized RelationNetwork-object\n","    support_x (torch tensor): data for support set\n","    support_y (torch tensor): labels for support set\n","    query_x (torch tensor): data for query set\n","    num_classes (int): number of classes in the given task\n","    support_num_per_class (int): number of support samples per class in support set\n","    seq_length (int): length of sequence\n","    num_units_lstm_encoder (int): number of hidden units in the LSTM-Cells of the feature_encoder\n","  Returns:\n","    relations (torch tensor): relation scores \n","  \"\"\"\n","\n","  # calculate support features by passing the support set through the feature_encoder-module \n","  support_features = feature_encoder(Variable(support_x).to(device))\n","  # calculate query features by passing the query set through the feature_encoder-module \n","  query_features = feature_encoder(Variable(query_x).to(device))\n","          \n","  #Prepare encoded query and support set for relations calculation (reshaping, transposing)\n","  support_features = support_features.view(num_classes, support_num_per_class, seq_length, num_units_lstm_encoder)\n","  support_features = torch.sum(support_features, 1).squeeze(1)\n","  support_features_ext = support_features.unsqueeze(0).repeat(1,1,1,1).to(device)\n","  query_features_ext = query_features.unsqueeze(0).repeat(num_classes,1,1,1)\n","  query_features_ext = torch.transpose(query_features_ext,0,1).to(device)\n","\n","  # concatenated encoded query & support set\n","  relation_pairs = torch.cat((support_features_ext,query_features_ext),3).view(-1,seq_length, num_units_lstm_encoder*2).to(device)\n","\n","  # calculate relations by passing the concatenated query & support set through the relation_network-module \n","  relations = relation_network(relation_pairs).view(-1,num_classes).to(device)\n","\n","  return relations"]},{"cell_type":"markdown","metadata":{"id":"HIV7DHhNgChs"},"source":["# Step No 3 - Configurate your model."]},{"cell_type":"markdown","metadata":{"id":"7_1GjqLo4HCP"},"source":["As a third step, you have to configure the model.\n","\n","Define the number of classes you want your model to distinguish (5 or 10) in **N_WAY** and the number of support samples per class you want to provide to your model in **K_SHOT** (1, 2 or 5).\n","\n","Additionally, replace the placeholders in the list GESTURES with the classnames of your choice. Make sure they are comma-seperated and in between the quotation marks.\n","\n","Run the cell.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XhPtxglR9aLU"},"outputs":[],"source":["N_WAY = 5\n","K_SHOT = 1\n","\n","GESTURES = [\"Replace_this_String_with_the_Name_of_your_Gesture_Number_1\",\n","            \"Replace_this_String_with_the_Name_of_your_Gesture_Number_2\",\n","            \"Replace_this_String_with_the_Name_of_your_Gesture_Number_3\",\n","            \"Replace_this_String_with_the_Name_of_your_Gesture_Number_4\",\n","            \"Replace_this_String_with_the_Name_of_your_Gesture_Number_5\"] \n","\n","feature_encoder, relation_network = initialize_model(N_WAY, K_SHOT, GESTURES)"]},{"cell_type":"markdown","metadata":{"id":"nv4OOG_JLojT"},"source":["# Step No 4 - Create the model's support set."]},{"cell_type":"markdown","metadata":{"id":"KIhfJwkT6VsS"},"source":["In order to perform predictions, the model needs some samples (defined by K_SHOT) of each class. In order to do this, run the following cell.\n","\n","The application will loop over every class and ask you to perform the class's hand gesture K_SHOT times. Further details will be explained during runtime. Afterwards, the data gets processed, which may take a while."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N0AU99VN1vOI"},"outputs":[],"source":["support_set = create_support_set(N_WAY, K_SHOT, GESTURES)"]},{"cell_type":"markdown","metadata":{"id":"BUPGiXSfLsjD"},"source":["# Step No 5 - Perform predictions!"]},{"cell_type":"markdown","metadata":{"id":"2_EX0b_M9gxm"},"source":["Everything is set up! In order to perform a prediction, run the following cell and follow the instructions."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"grsemtQcRy_T"},"outputs":[],"source":["perform_prediction(support_set, GESTURES, feature_encoder, relation_network)"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["HnUoZav9Lzf-"],"name":"mvp_gcolab.ipynb","provenance":[{"file_id":"1aiO9Wro9YX2Pcgmq4O5GpbyyTIY_Cm6v","timestamp":1649868727453},{"file_id":"https://github.com/emilyxxie/colab_utils_and_snippets/blob/master/video_webcam_snippets.ipynb","timestamp":1649744272716}]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"nbformat":4,"nbformat_minor":0}